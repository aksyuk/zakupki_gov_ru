---
title: "Руководство по загрузке данных"
output: word_document
---

<!-- 
output: 
    pdf_document:
        latex_engine: xelatex
        includes:
            in_header: "preamble.tex"
lang: 'ru-RU'
--> 

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = T, eval = F)

```

Общие замечания:   

* Далее все пути к папкам указаны относительно директории репозитория, `zakupki_gov_ru`.   

* Большинство таблиц содержит данные в сложных форматах типа дат или ID, которые записаны цифрами, но могут начинаться с 0 и содержат фиксированное количество символов. Поэтому таблица обычно записывается и читается вместе с таблицей метаданных (имя заканчивается на "_META"). В таблице метаданных указаны классы столбцов, она собирается автоматически.   



## 1 Создание переменных рабочего пространства и структуры папок с данными   

Скрипт **`parser-ftp-01_Setup-fz44.R`** делает две вещи:   

1. Создаёт глобальные переменные: период времени, регион, все пути к папкам с данными, url для загрузки данных.   

2. Загружает список регионов с сервера госзакупок. Это можно сделать 1 раз и дальше читать из `./data/reference/regions_list.txt`.   

3. Создаёт папки для загрузки данных, если не существуют.   


### Последовательность действий  

1. Открыть скрипт `parser-ftp-01_Setup-fz44.R`.  
2. Сделать папку репозитория `zakupki_gov_ru` активной (можно как папку со скриптом): Session >> Set Working Directory >> To Source File Location    
3. Задать год и месяц выгрузки данных (например, январь 2020):  

```{r}
# январь 2020
sYEAR <- paste0(rep(2020, 1), # это год, повторяем столько раз,
                              # сколько будет месяцев
                formatC(1, # это номер месяца (январь)
                        width = 2, flag = '0'))

```

4. Раскомментировать часть названия одного региона (например, Башкирия) для поиска в общем списке регионов:

```{r}
# часть названия региона для поиска
# 01
srch.reg <- 'Bashk'

```

5. Если список регионов ещё не создан (т.е. при первом прогоне скрипта), загрузить его с сервера. Раскомментировать и запустить строки:  

```{r}
# /////////////////////////ВВОД ДАННЫХ В КОНСОЛЬ////////////////////////////////
# создаём список вариантов для выбора
vars <- data.frame(n = 1:2, txt = c('Перезагрузить с ftp', 
                                    'Прочитать сохранённый'))
# показываем варианты пользователю
message(paste0('Загрузка списка регионов:\n',
               paste0(apply(vars, 1, function(x){paste0(x, collapse = '. ')}),
                      collapse = '\n')))

# в этой строке читаем выбор пользователя
prompt.load.reg.list <- readline('Введите номер опции:')

```

Запустить кусок кода "Список директорий с регионами" (до следующего заголовка "Структура директорий папки с данными"). Убедиться, что в папке `./data/reference/` обновился файл `regions_list.txt`.   

При повторных прогонах раскомментировать строку с быстрой опцией:   

```{r}
# /////////////////////////ВВОД ДАННЫХ В КОНСОЛЬ////////////////////////////////
# создаём список вариантов для выбора
# vars <- data.frame(n = 1:2, txt = c('Перезагрузить с ftp', 
#                                     'Прочитать сохранённый'))
# показываем варианты пользователю
# message(paste0('Загрузка списка регионов:\n',
#                paste0(apply(vars, 1, function(x){paste0(x, collapse = '. ')}),
#                       collapse = '\n')))

# в этой строке читаем выбор пользователя
# prompt.load.reg.list <- readline('Введите номер опции:')

# быстрая опция 
prompt.load.reg.list <- 2

```

6. Запустить кусок кода с заголовка "Структура директорий папки с данными" по заголовок "Выбрать директорию вручную или создать новую".   

7. Раскомментировать строки:   

```{r}
# /////////////////////////ВВОД ДАННЫХ В КОНСОЛЬ////////////////////////////////
message('Выберите выгрузку:\n', msg)
prompt.load.sample <- readline('Введите номер опции:')
# быстрая опция: новая выгрузка
# prompt.load.sample <- n.dirs + 1
# быстрая опция: выбрать по названию региона
# prompt.load.sample <- n.dirs + 2
# /////////////////////КОНЕЦ ВВОДА ДАННЫХ В КОНСОЛЬ/////////////////////////////

```

При первом прогоне в консоли выбрать вариант 1.   

8. Запустить код до "все типы процедур". Раскомментировать строки:   

```{r}
# все типы процедур
#  на самом деле там пока только электронные аукционы
all.proc.types <- read.csv2(paste0(sRefPath, 'dt_procedure_types.csv'),
                            stringsAsFactors = F, fileEncoding = 'cp1251')

msg <- paste0(1:nrow(all.proc.types), '. ', all.proc.types$procedureType)

# /////////////////////////ВВОД ДАННЫХ В КОНСОЛЬ////////////////////////////////
message('Выберите процедуры:\n', msg)
# prompt.proc.type <- readline('Введите номер опции:')
# быстрая опция
prompt.proc.type <- 1
# /////////////////////КОНЕЦ ВВОДА ДАННЫХ В КОНСОЛЬ/////////////////////////////

```

Запустить код до конца файла. В папке с загрузкой появится файл `README.txt`.   



## 2 Загрузка файлов с сервера     

Скрипт **`parser-ftp-02_Load-Unzip-and-Index-fz44.R`**:   

1. Выдаёт в консоль `my.region$url`, с которого надо вручную загрузить архивы в папку текущей загрузки >> archives. Название текущей папки загрузки лежит в переменной `sDataSamplePath`.   

2. Дальше надо вручную скачать архивы с сервера.   

3. Следующая часть скрипта создаёт файл для распаковки архивов, но только под linux.   

4. После распаковки архивов в `xmls` делается индексация. Цель - получить список уникальных номеров закупок (сохраняется в объект `all.xmls` и записывается в `./data/<текущая загрузка>/xlms_names_list.csv`)


### Скачивание архивов с помощью FileZilla

1. Создать подключение к серверу `ftp.zakupki.gov.ru`, логин `free`, пароль `free`.    

```{r, out.width = '7in', eval = T, echo = F}
knitr::include_graphics('./pic/pic-01.png')

```

2. В поле "Локальный сайт" выбрать папку из переменной `sDataSamplePath`, и внутри неё `archives`. В поле "Удалённый сайт" ввести кусок url из переменной `my.region$url`, начиная с "/fcs_regions/" и дальше. Для Башкортостана будет "/fcs_regions/Bashkortostan_Resp/".    

```{r, out.width = '7in', eval = T, echo = F}
knitr::include_graphics('./pic/pic-02.PNG')

```

3. Скачать из папок: `contracts`, `protocols`, `notifications` -- архивы за нужный период времени. В именах архивов в этих папках закодирован год и месяц в формате "ГГГГММ", выбираем нужные.         

```{r, out.width = '7in', eval = T, echo = F}
knitr::include_graphics('./pic/pic-03.png')

```

4. Распаковать архивы из папки `archives` в `xmls`. В скрипте `parser-ftp-02_Load-Unzip-and-Index-fz44.R` есть код для создания скрипта для ubuntu для разархиварования, для Windows такого скрипта нет. Будет здорово, если в том числе и эта задача будет решена в рамках практики.   



## 3 Индексация xml-файлов     

Это всё ещё скрипт **`parser-ftp-02_Load-Unzip-and-Index-fz44.R`**, часть после заголовка "ИНДЕКСАЦИЯ" до конца. Запускаем скрипт, в консоли появятся сообщения о том, сколько уникальных номеров извещений обнаружено.    

В конце скрипта создаётся важный вектор `all.xmls` -- это имена всех файлов xml в распакованных архивах (папка `./data/raw/<папка загрузки>/xmls`). Поскольку имён может быть очень много, а перебирать директорию каждый раз, оцень долго, этот вектор с именами записывается в файл `./data/raw/<папка загрузки>/xlms_names_list.csv`.    



## 4 Разбор xml-файлов   

Скрипт `parser-ftp-03_Parse-Loaded_XML-fz44.R`. Что он делает:   

1. На основе вектора с именами всех xml-файлов (объект `all.xmls`) и типа процедуры (объект `lProcedureToScrap$procedureCode`) создаёт таблицу с префиксами, ID извещения и уникальным id всех файлов для разбора (таблица `DT.proc.index`, которая записывается в файл `./data/raw/<папка загрузки>/csv/DT_all_xml_files_index.csv`). Это делается функцией `uf.make.file.index.for.proc()` по следующей логике:     

* берём имена `all.xmls`, которые имеют вид: "<префикс файла>_<ID закупки, 19 цифр>_<id файла, 8-9 цифр>.xml";   

* фильтруем по типу процедуры: электронный аукцион (значение из `lProcedureToScrap$procedureCode`);    

* вырезаем из оставшегося ID закупки (notification ID);     

* фильтруем имена xml-файлов по полученным ID закупки. Эти ID уникальны, и повторяются в названиях файлов для разных этапов жизни закупки. Таким образом у нас получатся файлы с разными префиксами, но ID будут относиться к тем закупкам, которые шли в формате электронных аукционов.     

2. Основываясь на xpath запросах из файла `./data/reference/df_xml_patterns`, разбираем xml-файлы. В процессе может оказаться, что один тег в xml-файле встречается несколько раз. Тогда все значения тега склеиваются в одну строку, через символ решётки. На данном этапе для каждого префикса xml-файла получается своя таблица, в которой одна строка это сведения, извлечённые из одного файла xml. Файлы .csv с результатами разбора записывается в папку `./data/raw/<папка загрузки>/csv/<код процедуры>`.    

3. Затем, из-за того что мы склеивали значения повторяющихся тегов в одно, надо нормализовать таблицы с выборками из xml. Поскольку таблицы выходят большие, они режутся на части и нормализуются по частям. Промежуточные результаты пишутся в папку `tmp`, затем склеиваются вместе. Положение осложняется тем, что в некоторых таблицах с результатами разбора xml-файлов представлены сведения о разных аспектах закупки. Если говорить в терминах базы данных, там соединены таблицы с разными ключами, поэтому расклеивать, например, `DT_fcsNotificationEA44` приходится в несколько этапов. Это делается в разделе скрипта под заголовком "4. ЧИСТИМ RAW-ТАБЛИЦЫ". Надо прогнать эту часть скрипта и посмотреть, какие появятся ошибки. Они могут быть связаны с тем, что схема xml изменилась, либо появились какие-то новые исключения, и тогда придётся корректировать xpath запросы.   


## Подготовка данных для моделей   

Скрипты `parser-ftp-04_Prepare-Data-for-Models.R` и `parser-ftp-04_Prepare-Data-for-Models_2.R` делают на основе очищенных таблиц таблицу переменными для модели. На данных момент есть две спецификации модели, поэтому скрипта 2.   
